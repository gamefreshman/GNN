# Semi-Supervised Classification with Graph Convolutional Networks

## 逻辑主线

![img](https://i-blog.csdnimg.cn/blog_migrate/ec57dea361baf0f216d23ad179cc3ce9.png)



1. 由于经典卷积神经网络不能处理非结构化数据，因此考虑如何将卷积操作拓展到图结构数据中
   1. 经典卷积：处理固定维度、输入结构有序



## UnKnown or Incomprehensible description

- We present a **scalable** approach for **semi-supervised** learning on graph-structured data that is based on an efficient **variant** of convolutional neural networks which operate directly on graphs. 
- We motivate the choice of our convolutional architecture via a **localized** **first-order approximation** of **spectral graph** convolutions.
- Our model **scales linearly** in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. 
- where label information is **smoothed** over the graph via some form of **explicit** graph-based **regularization**
- **w.r.t.**
- **differentiable** function
- $ \Delta = D - A $ denotes the unnormalized graph Laplacian of an undirected graph
- an **adjacency** matrix $ A \in \mathbb{R}^{N \times N} $ (**binary or weighted**) and a degree matrix $D_{ii} = \sum_{j} A_{ij}$ 
- **formulation** 
- the assumption that connected nodes in the graph are likely to share the same label, however, might **restrict** modeling capacity, as graph edges need **not** necessarily encode node similarity, **but** could contain additional information. 
- thereby **avoiding** explicit graph-based regularization in the loss function
- **Conditioning** $f(\cdot)$on the adjacency matrix of the graph will allow the model to **distribute** gradient information from the supervised loss $\mathcal{L}_0$ and will **enable** it to learn representations of nodes both with and without labels.
- layer-**wise**
- how how it can be **motivated** from a first-order approximation of spectral graph convolutions

## Related Works

### 如何将卷积操作拓展到图结构数据中？

#### 卷积

- 骰子举例，点数概率

#### 谱域卷积

##### 正定矩阵和半正定矩阵

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/91ecf9771192d3469024f42dbcece77a.png)

##### 拉普拉斯矩阵

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/cc6cdfa1d74ccf74e551b52482e9dc6c.png#pic_center)

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/e470b9c3e3ab33a1205fa78afe4a5883.png#pic_center)

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/7d7de943cca33338d3054d8db0ba1f7f.png#pic_center)

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/c3f1e8d1bf13827448388b10448a9fe6.png#pic_center)

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/86cad7698410a16a41099d876c092c9b.png#pic_center)

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/2b80e0bd2dba268f46c71414307cea4d.png#pic_center)

##### 傅里叶变换

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/50ae17fd8f59953ef9dcd221d21600cc.png#pic_center)

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/87ebc57b01d000c0e5507a17419a369d.png#pic_center)

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/906f16ee2b3bb12e0e85618cf4880a99.png#pic_center)

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/34cc9858b60a8f6a0fcc7ad88ef81ba9.png#pic_center)

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/1224f701acb9a54f69fc1f1dd24d9356.png#pic_center)

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/f812842a0b093429ba06a826cc6a1ac3.png#pic_center)

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/0c81717a822619a2c97296cc46fb8fa1.png#pic_center)

#### 谱域图卷积的缺陷

![img](https://i-blog.csdnimg.cn/blog_migrate/26dcd570286835d50712588678b1a7c5.png)

![img](https://i-blog.csdnimg.cn/blog_migrate/28949bf4d07babdd93cb3d8007e86015.png)

![img](https://i-blog.csdnimg.cn/blog_migrate/99c45d2dd126039feb9589297741ebdf.png)

